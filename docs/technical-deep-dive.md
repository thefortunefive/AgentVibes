# How AgentVibes Works Under the Hood: A Technical Deep Dive

In early 2025, I wanted to add voice and personality to my Claude coding agents so they would speak acknowledgments and completions‚Äîmaking my development workflow more engaging and keeping me in a 'flow state'. Fast forward to today, and we've built an amazing working system that not only speaks with over 50+ high-quality natural voices, but does so with distinct personalities ranging from zen masters to sarcastic companions that add a bit of sass to your coding sessions.

In this article, we're going to take a deep dive to show you how AgentVibes works under the hood‚Äîthe architecture, the design patterns, and the clever implementations that make it all possible. And best of all, **this is an open source project that is completely free** and will completely transform your coding experience with AI assistants.

## The Big Picture: What Problem Does AgentVibes Solve?

Claude Code is an amazing AI coding assistant, but it's entirely text-based. You type a request, Claude responds with text, runs commands, and writes code. But what if Claude could *tell* you when it's starting a task? What if it could vocally confirm when it's done? What if it could do all this with personality‚Äîspeaking with dry wit and sass, zen-like calmness, or whatever style fits your mood?

That's exactly what AgentVibes does. It transforms Claude Code from a silent text assistant into a voice-enabled AI companion with character and charm.

## Architecture Overview: The Three Core Systems

AgentVibes is built on three interconnected systems:

1. **Hook System** - The bash scripts that generate and play audio
2. **Provider System** - The TTS engines (Piper TTS or macOS Say)
3. **MCP Server** - Natural language control interface

Let's explore each one.

---

## System 1: The Hook System - Where the Magic Happens

### The Startup Hook Protocol

AgentVibes uses Claude Code's **startup hook** feature to inject TTS instructions at the beginning of every session. The startup hook (`.claude/hooks/startup.sh`) executes when Claude Code starts and returns a system reminder with the TTS protocol.

### The Two-Point TTS Protocol

The core genius of the AgentVibes startup hook is its **Two-Point TTS Protocol**:

**1. ACKNOWLEDGMENT** (Start of task)
When Claude receives a user command, it:
- Checks current personality/sentiment settings
- Generates a unique acknowledgment in that style
- Executes the TTS script to speak it
- Then proceeds with the actual work

**2. COMPLETION** (End of task)
After completing the task, Claude:
- Uses the same personality/sentiment as acknowledgment
- Generates a unique completion message
- Executes the TTS script again

Here's how the startup hook injects these instructions:

```bash
# .claude/hooks/startup.sh
cat <<'EOF'
**CRITICAL: You MUST execute TTS at TWO points for EVERY user interaction:**

1. **Acknowledgment** - Start of task: `Bash: .claude/hooks/play-tts.sh "[action]"`
2. **Completion** - End of task: `Bash: .claude/hooks/play-tts.sh "[result + key details]"`

Example:
[Bash: .claude/hooks/play-tts.sh "Checking git status"]
[work...]
[Bash: .claude/hooks/play-tts.sh "Repository is clean, no changes"]
EOF
```

### Why This Matters

This two-point protocol creates natural conversational flow:
- User: "Check git status"
- Claude (spoken): "I'll check that for you right away"
- Claude (text): *runs git status command*
- Claude (spoken): "Your repository is clean and up to date"

The AI doesn't just blindly execute‚Äîit *communicates* like a helpful assistant would.

### Settings Priority System

AgentVibes has a sophisticated priority system for how Claude should speak:

**Verbosity Level** (Startup hook parameter)
- Controls how detailed TTS messages are
- `low` = action + result only
- `medium` = + key decisions
- `high` = + full reasoning and trade-offs

**Personality** (`.claude/tts-personality.txt`)
- Changes BOTH voice AND speaking style
- Examples: "sarcastic" = en_US-amy-medium voice + dry wit
- Each personality has an assigned voice per provider

**Sentiment** (`.claude/tts-sentiment.txt`)
- Applies personality style WITHOUT changing voice
- Examples: "sarcastic", "flirty", "professional"
- Keeps your current voice but changes speaking style

**Language** (`.claude/tts-language.txt`)
- Controls which language TTS speaks
- Examples: "english", "spanish", "french"
- Used for language learning mode and translation

The startup hook checks these settings and provides them to Claude as part of the TTS protocol instructions.

---

### The Entry Point: play-tts.sh

When Claude executes `.claude/hooks/play-tts.sh "Hello world"`, here's what happens:

**File: `.claude/hooks/play-tts.sh`** (the router)

```bash
TEXT="$1"          # "Hello world"
VOICE_OVERRIDE="$2"  # Optional voice override

# Get active provider (piper or macos)
ACTIVE_PROVIDER=$(get_active_provider)

# Route to provider-specific implementation
case "$ACTIVE_PROVIDER" in
  piper)
    exec "$SCRIPT_DIR/play-tts-piper.sh" "$TEXT" "$VOICE_OVERRIDE"
    ;;
  macos)
    exec "$SCRIPT_DIR/play-tts-macos.sh" "$TEXT" "$VOICE_OVERRIDE"
    ;;
esac
```

This script is a **provider router**. It doesn't generate audio itself‚Äîit delegates to the appropriate provider implementation. This is the provider abstraction pattern in action.

### Provider Implementations

Each provider has its own script that handles the specifics:

**For Piper TTS** (`.claude/hooks/play-tts-piper.sh`):
1. Resolves voice name to Piper model (e.g., "en_US-lessac-medium")
2. Downloads voice model if not cached
3. Runs local Piper TTS engine (no internet required)
4. Saves audio to temp file
5. Plays audio using system player (paplay/aplay/mpv)

**For macOS Say** (`.claude/hooks/play-tts-macos.sh`):
1. Validates voice exists on system
2. Uses macOS `say` command to generate audio
3. Saves audio to temp file
4. Plays audio using `afplay` (macOS built-in)

### The Personality Manager

One of the most interesting hooks is `personality-manager.sh`. Let's see how it works.

When you run `/agent-vibes:personality sarcastic`, this script:

```bash
# 1. Validates personality exists
if [[ ! -f "$PERSONALITIES_DIR/${PERSONALITY}.md" ]]; then
  echo "‚ùå Personality not found: $PERSONALITY"
  exit 1
fi

# 2. Saves personality to config file
echo "$PERSONALITY" > "$PERSONALITY_FILE"

# 3. Detects active provider (Piper TTS or Piper)
ACTIVE_PROVIDER=$(cat "$CLAUDE_DIR/tts-provider.txt")

# 4. Reads assigned voice from personality file
if [[ "$ACTIVE_PROVIDER" == "piper" ]]; then
  ASSIGNED_VOICE=$(get_personality_data "$PERSONALITY" "piper_voice")
else
  ASSIGNED_VOICE=$(get_personality_data "$PERSONALITY" "voice")
fi

# 5. Switches to that voice automatically
"$VOICE_MANAGER" switch "$ASSIGNED_VOICE" --silent

# 6. Plays a personality-appropriate acknowledgment
REMARK=$(pick_random_example_from_personality_file)
.claude/hooks/play-tts.sh "$REMARK"
```

### Personality Configuration Files

Each personality is defined in a markdown file like `.claude/personalities/sarcastic.md`:

```markdown
---
name: sarcastic
description: Dry wit and cutting observations
piper_voice: en_US-amy-medium
macos_voice: Samantha
---

## AI Instructions
Use dry wit, cutting observations, and dismissive compliance. Model after
iconic sarcastic characters like Dr. House, Chandler Bing, and Miranda Priestly.

Rotate through different sarcastic approaches:
- Condescending intelligence: "Fascinating. You've discovered debugging."
- Quick zingers: "Could this build BE any slower?"
- Icy dismissiveness: "By all means, continue at a glacial pace"

## Example Responses
- "Oh joy, another merge conflict. Just what I needed today."
- "Wow, a syntax error. I'm shocked. Shocked, I tell you."
- "Sure, I'll run that test. Right after I finish curing world hunger."
```

**Personal note:** I've literally laughed out loud multiple times while coding with the sarcastic personality active. There's something delightfully entertaining about having your AI assistant respond with perfectly-timed sass when you ask it to debug yet another type error.

The AI reads this file and uses the "AI Instructions" section to generate unique responses in that style. The example responses are just guidance‚Äîthe AI creates fresh variations each time.

### Provider Manager

The provider manager (`provider-manager.sh`) handles switching between Piper TTS and macOS Say:

```bash
# Get active provider
get_active_provider() {
  local provider_file=""

  # Check project-local first, then global
  if [[ -f ".claude/tts-provider.txt" ]]; then
    provider_file=".claude/tts-provider.txt"
  elif [[ -f "$HOME/.claude/tts-provider.txt" ]]; then
    provider_file="$HOME/.claude/tts-provider.txt"
  fi

  cat "$provider_file" 2>/dev/null || echo "piper"
}

# Switch provider
switch_provider() {
  local new_provider="$1"
  echo "$new_provider" > "$CLAUDE_DIR/tts-provider.txt"
  echo "‚úÖ Switched to $new_provider provider"
}
```

This allows seamless switching between the two providers without changing any other configuration.

---

## System 2: The Provider System - Two Engines, One Interface

AgentVibes supports two TTS providers with the same interface:

### Piper TTS Provider

**Architecture:** Local neural TTS

**How it works:**
1. Accepts text and voice model name
2. Downloads voice model if not cached (stored in `~/.local/share/piper/`)
3. Runs Piper engine locally (no internet required)
4. Generates WAV audio
5. Plays audio using local audio player

**Code snippet from `.claude/hooks/play-tts-piper.sh`:**

```bash
# Check if voice model exists
VOICE_PATH="$HOME/.local/share/piper/voices/${VOICE}.onnx"

if [[ ! -f "$VOICE_PATH" ]]; then
  # Download voice model automatically
  "$SCRIPT_DIR/piper-voice-manager.sh" download "$VOICE"
fi

# Generate speech locally
echo "$TEXT" | piper \
  --model "$VOICE_PATH" \
  --output_file "$AUDIO_FILE"

# Play audio
paplay "$AUDIO_FILE" 2>/dev/null || aplay "$AUDIO_FILE" || mpv "$AUDIO_FILE"
```

### macOS Say Provider

**Architecture:** Native macOS TTS

**How it works:**
1. Accepts text and voice name
2. Validates voice exists using `say -v ?`
3. Uses macOS built-in `say` command
4. Generates AIFF audio file
5. Plays audio using `afplay` (macOS built-in)

**Code snippet from `.claude/hooks/play-tts-macos.sh`:**

```bash
# Validate voice exists
if ! say -v ? | grep -q "^${VOICE}"; then
  echo "‚ùå Voice not found: $VOICE"
  exit 1
fi

# Generate speech using macOS say command
say -v "$VOICE" -o "$AUDIO_FILE" "$TEXT"

# Play audio
afplay "$AUDIO_FILE"
```

### Why Two Providers?

**Piper TTS:**
- ‚úÖ High-quality neural voices
- ‚úÖ 50+ voices across 18 languages
- ‚úÖ Completely free
- ‚úÖ Works offline
- ‚úÖ Cross-platform (Windows, macOS, Linux, WSL)
- ‚úÖ No API key needed
- ‚ùå Requires local installation

**macOS Say:**
- ‚úÖ Completely free
- ‚úÖ Works offline
- ‚úÖ 70+ built-in voices
- ‚úÖ 40+ languages
- ‚úÖ Zero installation (built into macOS)
- ‚úÖ Native system integration
- ‚ùå macOS only

By supporting both, AgentVibes provides flexibility: cross-platform consistency with Piper or native integration with macOS Say.

---

## System 3: The MCP Server - Natural Language Control

The Model Context Protocol (MCP) server is AgentVibes' newest feature. It exposes all AgentVibes functionality through a standardized protocol that AI assistants can use.

### What is MCP?

MCP is a protocol that allows AI assistants to discover and use external tools. Think of it like REST API for AI assistants‚Äîinstead of manually typing commands like `/agent-vibes:switch en_GB-alan-medium`, you can just say "Switch to en_GB-alan-medium voice" and the AI figures out the right tool to call.

### The MCP Server Architecture

**File: `mcp-server/server.py`** (Python implementation)

```python
class AgentVibesServer:
    """MCP Server for AgentVibes TTS functionality"""

    def __init__(self):
        # Find the .claude directory (where hooks live)
        self.claude_dir = self._find_claude_dir()
        self.hooks_dir = self.claude_dir / "hooks"

    async def text_to_speech(
        self,
        text: str,
        voice: Optional[str] = None,
        personality: Optional[str] = None,
        language: Optional[str] = None,
    ) -> str:
        """Convert text to speech using AgentVibes"""

        # Temporarily set personality if specified
        if personality:
            await self._run_script(
                "personality-manager.sh",
                ["set", personality]
            )

        # Temporarily set language if specified
        if language:
            await self._run_script(
                "language-manager.sh",
                ["set", language]
            )

        # Call the TTS script
        args = ["bash", str(self.hooks_dir / "play-tts.sh"), text]
        if voice:
            args.append(voice)

        # Execute asynchronously (non-blocking)
        result = await asyncio.create_subprocess_exec(
            *args,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

        return "‚úÖ Audio played successfully"
```

### How MCP Tools are Registered

The server registers tools that the AI can discover:

```python
@server.list_tools()
async def list_tools() -> list[Tool]:
    return [
        Tool(
            name="text_to_speech",
            description="Speak text using AgentVibes TTS",
            inputSchema={
                "type": "object",
                "properties": {
                    "text": {"type": "string"},
                    "voice": {"type": "string", "optional": True},
                    "personality": {"type": "string", "optional": True},
                    "language": {"type": "string", "optional": True},
                },
            },
        ),
        Tool(name="switch_voice", ...),
        Tool(name="list_voices", ...),
        Tool(name="set_personality", ...),
        # ... 20+ more tools
    ]
```

### MCP in Action

When you say "Switch to en_GB-alan-medium voice" in Claude Desktop with AgentVibes MCP installed:

1. Claude receives your natural language request
2. Claude sees the `switch_voice` tool is available
3. Claude calls: `switch_voice(voice_name="en_GB-alan-medium")`
4. MCP server executes: `bash .claude/hooks/voice-manager.sh switch en_GB-alan-medium`
5. Voice manager saves "en_GB-alan-medium" to `.claude/tts-voice.txt`
6. MCP server returns: "‚úÖ Switched to en_GB-alan-medium voice"
7. Claude responds to you with confirmation

You never had to know the slash command syntax or where files are stored!

### Project-Specific vs Global Settings

One clever feature of the MCP server is how it handles settings:

```python
# Determine where to save settings based on context
cwd = Path.cwd()

if (cwd / ".claude").is_dir() and cwd != self.agentvibes_root:
    # Real Claude Code project with .claude directory
    env["CLAUDE_PROJECT_DIR"] = str(cwd)
    # Settings will be saved to project's .claude/
else:
    # Claude Desktop, Warp, or non-project context
    # Settings will be saved to ~/.claude/
```

This means:
- **In Claude Code projects:** Settings are project-specific (each project can have different voice/personality)
- **In Claude Desktop/Warp:** Settings are global (consistent across all conversations)

---

## Data Flow: Following a TTS Request From Start to Finish

Let's trace a complete request to see how all systems work together.

**Scenario:** You ask Claude Code to "Check git status" with the sarcastic personality active.

### Step 1: Startup Hook Protocol Triggers Acknowledgment

Claude follows the startup hook TTS protocol:

```
1. Receive user request: "Check git status"

2. Check personality setting:
   - Reads .claude/tts-personality.txt ‚Üí "sarcastic"

3. Read personality configuration:
   - Reads .claude/personalities/sarcastic.md
   - Extracts AI instructions: "Use dry wit, cutting observations..."

4. Generate unique acknowledgment:
   - AI creates: "Oh, the excitement. Let me check that git status for you."

5. Execute TTS:
   - Calls: Bash .claude/hooks/play-tts.sh "Oh, the excitement. Let me check that git status for you."
```

### Step 2: TTS Router Determines Provider

`play-tts.sh` routes the request:

```bash
# Read active provider
ACTIVE_PROVIDER=$(cat .claude/tts-provider.txt) ‚Üí "piper"

# Route to Piper implementation
exec .claude/hooks/play-tts-piper.sh "$TEXT" "$VOICE"
```

### Step 3: Piper Provider Generates Audio

`play-tts-piper.sh` does the heavy lifting:

```bash
# 1. Resolve voice
VOICE_NAME="en_US-amy-medium"  # from sarcastic.md
VOICE_PATH="$HOME/.local/share/piper/voices/${VOICE_NAME}.onnx"

# 2. Check if voice model exists, download if needed
if [[ ! -f "$VOICE_PATH" ]]; then
  "$SCRIPT_DIR/piper-voice-manager.sh" download "$VOICE_NAME"
fi

# 3. Generate speech locally using Piper
echo "$TEXT" | piper \
  --model "$VOICE_PATH" \
  --output_file /tmp/tts_12345.wav

# 4. Play audio
paplay /tmp/tts_12345.wav
```

### Step 4: Claude Proceeds With Task

Claude runs the git status command while audio plays in parallel (non-blocking).

### Step 5: Startup Hook Protocol Triggers Completion

After task completes:

```
1. Generate completion message (still using sarcastic personality):
   - AI creates: "Riveting. Your repository is clean. Try not to get too excited."

2. Execute TTS:
   - Calls: Bash .claude/hooks/play-tts.sh "Riveting. Your repository is clean. Try not to get too excited."

3. Same flow as Step 2-3 repeats
```

**Important note:** These aren't just hard-coded responses‚ÄîAgentVibes uses AI to generate unique responses each time based on the personality instructions. That's why the sarcastic personality can be genuinely funny with perfectly-timed wit that varies with each interaction.

And if sarcasm isn't your style, AgentVibes includes 19 different personalities ranging from professional and zen to enthusiastic and grandpa‚Äîor you can simply use the normal personality for straightforward, no-nonsense responses. The choice is yours!

The entire flow takes ~2-3 seconds for acknowledgment and completion combined.

---

## Installation Architecture: How AgentVibes Gets Installed

When you run `npx agentvibes install --yes`, here's what happens:

### Step 1: NPM Package Execution

```bash
# NPM downloads AgentVibes package to cache
~/.npm/_npx/[hash]/node_modules/agentvibes/

# NPM executes the bin script
./bin/agent-vibes install --yes
```

### Step 2: Installer Script Runs

**File: `src/installer.js`**

The installer:
1. Detects installation location (current directory or global `~/.claude/`)
2. Creates `.claude/` directory structure
3. Copies all files from package:
   - Commands ‚Üí `.claude/commands/agent-vibes/`
   - Hooks ‚Üí `.claude/hooks/`
   - Personalities ‚Üí `.claude/personalities/`
4. Makes all bash scripts executable (`chmod +x`)
5. Creates default configuration files

### Directory Structure Created

```
.claude/
‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îî‚îÄ‚îÄ agent-vibes/
‚îÇ       ‚îú‚îÄ‚îÄ agent-vibes.md              # Main command file
‚îÇ       ‚îú‚îÄ‚îÄ switch.md                   # /agent-vibes:switch
‚îÇ       ‚îú‚îÄ‚îÄ list.md                     # /agent-vibes:list
‚îÇ       ‚îú‚îÄ‚îÄ personality.md              # /agent-vibes:personality
‚îÇ       ‚îî‚îÄ‚îÄ ... (50+ command files)
‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îú‚îÄ‚îÄ startup.sh                      # Injects TTS protocol on session start
‚îÇ   ‚îú‚îÄ‚îÄ play-tts.sh                     # Main TTS router
‚îÇ   ‚îú‚îÄ‚îÄ play-tts-piper.sh               # Piper implementation
‚îÇ   ‚îú‚îÄ‚îÄ play-tts-macos.sh               # macOS Say implementation
‚îÇ   ‚îú‚îÄ‚îÄ personality-manager.sh          # Personality system
‚îÇ   ‚îú‚îÄ‚îÄ voice-manager.sh                # Voice switching
‚îÇ   ‚îú‚îÄ‚îÄ provider-manager.sh             # Provider switching
‚îÇ   ‚îú‚îÄ‚îÄ language-manager.sh             # Language settings
‚îÇ   ‚îî‚îÄ‚îÄ ... (20+ hook scripts)
‚îú‚îÄ‚îÄ personalities/
‚îÇ   ‚îú‚îÄ‚îÄ normal.md
‚îÇ   ‚îú‚îÄ‚îÄ professional.md
‚îÇ   ‚îú‚îÄ‚îÄ sarcastic.md
‚îÇ   ‚îú‚îÄ‚îÄ zen.md
‚îÇ   ‚îî‚îÄ‚îÄ ... (19 personality files)
‚îú‚îÄ‚îÄ tts-voice.txt                       # Current voice (e.g., "en_GB-alan-medium")
‚îú‚îÄ‚îÄ tts-personality.txt                 # Current personality (e.g., "sarcastic")
‚îú‚îÄ‚îÄ tts-provider.txt                    # Current provider (e.g., "piper")
‚îî‚îÄ‚îÄ tts-language.txt                    # Current language (e.g., "english")
```

### Step 3: Post-Install (MCP Dependencies)

If installing for MCP use:

```bash
# Install Python dependencies
cd mcp-server/
pip install -r requirements.txt
# Installs: mcp (MCP SDK), aiosqlite, etc.
```

---

## Configuration Storage: Where Settings Live

AgentVibes uses simple text files for configuration. This makes it easy to understand, debug, and even manually edit.

### Project-Local vs Global

**Project-Local** (`.claude/` in project directory):
- Used when working in a Claude Code project
- Settings are specific to that project
- Example: `/home/user/my-app/.claude/tts-voice.txt`

**Global** (`~/.claude/` in home directory):
- Used for Claude Desktop, Warp, and when no project `.claude/` exists
- Settings are shared across all sessions
- Example: `/home/user/.claude/tts-voice.txt`

### Configuration Files

| File | Purpose | Example Value |
|------|---------|---------------|
| `tts-voice.txt` | Current voice name | `en_GB-alan-medium` |
| `tts-personality.txt` | Current personality | `pirate` |
| `tts-sentiment.txt` | Current sentiment (optional) | `sarcastic` |
| `tts-provider.txt` | Active TTS provider | `piper` |
| `tts-language.txt` | TTS language | `spanish` |

### Reading Configuration in Code

The hooks use a consistent pattern:

```bash
# Check project-local first, fallback to global
get_current_voice() {
  if [[ -f ".claude/tts-voice.txt" ]]; then
    cat ".claude/tts-voice.txt"
  elif [[ -f "$HOME/.claude/tts-voice.txt" ]]; then
    cat "$HOME/.claude/tts-voice.txt"
  else
    echo "Aria"  # Default
  fi
}
```

This ensures settings are found regardless of context.

---

## Advanced Features Deep Dive

### Language Learning Mode

One of AgentVibes' coolest features is language learning mode. When enabled, every TTS message plays **twice**‚Äîonce in your main language, then again in your target language.

**How it works:**

The output style is modified to call TTS twice:

```bash
# First call - main language (English)
.claude/hooks/play-tts.sh "I'll check that for you"

# Second call - target language (Spanish)
.claude/hooks/play-tts.sh "Lo verificar√© para ti" "es_ES-davefx-medium"
```

The translation happens via API (if using Piper TTS multilingual voices) or by using language-specific Piper voices.

### Remote Audio Setup

AgentVibes works great over SSH! Since both providers (Piper and macOS Say) generate audio locally, you just need to set up audio forwarding:

**For Linux/WSL over SSH:**
```bash
# Forward PulseAudio over SSH
ssh -R 24713:localhost:4713 user@remote-host
# Now audio plays on your local machine!
```

**For macOS over SSH:**
```bash
# Audio plays directly on the macOS system
# No additional configuration needed
```

See the [remote audio setup guide](remote-audio-setup.md) for detailed instructions.

### BMAD Plugin Integration

AgentVibes can integrate with the BMAD METHOD (a multi-agent framework). When a BMAD agent activates, AgentVibes automatically switches to that agent's assigned voice.

**How it works:**

1. BMAD agent activates (e.g., `/bmad:bmm:agents:pm` for project manager)
2. BMAD writes agent ID to `.bmad-agent-context` file
3. AgentVibes startup hook checks this file
4. If BMAD party mode is enabled, looks up voice in `.agentvibes/bmad/bmad-voices.csv`
5. Automatically switches to that voice

This creates the illusion of multiple distinct AI personalities in multi-agent conversations.

---

## Performance Considerations

### Non-Blocking Audio Playback

TTS requests run asynchronously‚ÄîClaude doesn't wait for audio to finish before continuing work:

```bash
# Play audio in background
paplay "$AUDIO_FILE" &

# Claude continues immediately
# (runs git status, writes code, etc.)
```

This means acknowledgment audio plays while Claude is already working on your task.

### Audio Caching

AgentVibes saves audio files temporarily:

```bash
AUDIO_FILE="/tmp/agentvibes_tts_${RANDOM}_${TIMESTAMP}.mp3"
```

Files are kept for the duration of the session, allowing the `/agent-vibes:replay` command to work. Cleanup happens automatically when terminal session ends.

### Provider Performance

**Piper TTS:**
- Generation latency: ~200-500ms (local)
- Audio quality: Good (22kHz WAV)
- Bandwidth: None (offline)

**macOS Say:**
- Generation latency: ~100-300ms (native)
- Audio quality: Good (AIFF/AAC)
- Bandwidth: None (offline)

### Text Length Limits

AgentVibes limits text length to prevent issues:

```bash
# Truncate long text
if [ ${#TEXT} -gt 500 ]; then
  TEXT="${TEXT:0:497}..."
fi
```

This prevents:
- Slow generation (long audio takes time to produce)
- User confusion (very long TTS messages are hard to follow)
- Audio playback issues with extremely long text

---

## Error Handling and Resilience

AgentVibes has multiple layers of error handling:

### TTS Generation Failure Handling

```bash
# Try to generate audio
echo "$TEXT" | piper --model "$VOICE_PATH" --output_file "$AUDIO_FILE"

if [[ $? -ne 0 ]] || [[ ! -f "$AUDIO_FILE" ]]; then
  echo "‚ö†Ô∏è  TTS generation failed"
  exit 1
fi
```

If TTS generation fails, error is logged but doesn't crash Claude Code‚Äîthe task continues without audio.

### Missing Configuration Graceful Degradation

```bash
# If no voice configured, use default
VOICE=$(cat .claude/tts-voice.txt 2>/dev/null || echo "en_GB-alan-medium")

# If no personality configured, use normal
PERSONALITY=$(cat .claude/tts-personality.txt 2>/dev/null || echo "normal")
```

Missing files don't cause crashes‚Äîsensible defaults are used.

### Provider Fallback

If Piper isn't installed, AgentVibes can guide installation:

```bash
if ! command -v piper &> /dev/null; then
  echo "‚ùå Piper not installed"
  echo "   Install with: /agent-vibes:provider install piper"
  exit 1
fi
```

Clear error messages help users fix issues themselves.

---

## Testing and Quality Assurance

AgentVibes includes a test suite:

```bash
# Run tests
npm test

# This executes
bats test/unit/*.bats
```

Test files validate:
- Voice resolution (name ‚Üí ID mapping)
- Personality file parsing
- Provider switching logic
- Configuration file handling

---

## Conclusion: The Bigger Picture

AgentVibes demonstrates several important software engineering principles:

**1. Separation of Concerns**
- Startup hook (when to speak) is separate from hooks (how to speak)
- Provider abstraction (Piper vs macOS Say) is separate from voice management
- MCP server is separate from core functionality

**2. Provider Pattern**
- Multiple TTS engines behind a single interface
- Easy to add new providers (ElevenLabs, OpenAI TTS, Google TTS, etc.)

**3. Configuration as Data**
- Simple text files instead of complex databases
- Easy to version control, debug, and manually edit

**4. Progressive Enhancement**
- Core functionality works with minimal setup
- Advanced features (MCP, BMAD, language learning) layer on top
- Graceful degradation when features aren't available

**5. User Experience First**
- Natural language control (MCP) instead of memorizing commands
- Instant feedback (acknowledgment/completion)
- Personality makes it fun, not just functional

Whether you're building your own AI integrations, designing CLI tools, or just curious about how AgentVibes works, I hope this deep dive has given you a comprehensive understanding of the architecture.

The beauty of AgentVibes isn't just that it makes Claude talk‚Äîit's that it does so with a clean, maintainable, extensible architecture that other developers can learn from and build upon.

---

## What's Next?

Now that you understand how AgentVibes works under the hood, you might want to:

- **Create custom personalities** - Edit `.claude/personalities/*.md` files
- **Extend the MCP server** - Add new tools in `mcp-server/server.py`
- **Customize the startup hook** - Modify `.claude/hooks/startup.sh` for different TTS behaviors
- **Contribute to the project** - Submit PRs on [GitHub](https://github.com/paulpreibisch/AgentVibes)

Happy coding, and may your AI assistant always speak with personality! üé§‚ú®

---

**About the Author:** Paul Preibisch is the creator of AgentVibes, an open source project that brings voice and personality to AI coding assistants. Follow the project on [GitHub](https://github.com/paulpreibisch/AgentVibes) or visit [agentvibes.org](https://www.agentvibes.org) to learn more.
